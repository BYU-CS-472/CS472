{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Lab\n",
    "In this lab, you will learn about Q-Learning. You will write a training function and learn how training affects performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and install any dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygame\n",
    "# !pip install numpy\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Frozen Lake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Get a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the environment\n",
    "\n",
    "We want our character to get to the present without falling in the ice holes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Initialize the Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")\n",
    "\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "print(\"Q-Table \\n\", Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to display the q-table values on the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table(q_table, env, use_best_action=False):\n",
    "    \"\"\"\n",
    "    Visually displays the Q-table values for a grid observation space using row-major order.\n",
    "    Automatically extracts and displays state types (F, H, G, etc.) from the Gym environment.\n",
    "    Highlights the best Q-value in each state.\n",
    "\n",
    "    Parameters:\n",
    "    - q_table: A numpy array of shape (num_states, num_actions), where each state has Q-values for [left, down, right, up].\n",
    "    - env: The OpenAI Gymnasium environment.\n",
    "    - use_best_action: If True, displays the best action in each state instead of the state label.\n",
    "    \"\"\"\n",
    "    # Extract grid shape from the environment\n",
    "    rows, cols = env.unwrapped.desc.shape\n",
    "    \n",
    "    # Extract state labels (F, H, G, etc.)\n",
    "    state_labels = list(env.unwrapped.desc.astype(str).flatten())  # No transposition needed\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(cols * 2.0, rows * 2.0))  # Larger cells for readability\n",
    "    ax.set_xlim(0, cols)\n",
    "    ax.set_ylim(0, rows)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_frame_on(False)\n",
    "    \n",
    "    action_labels = ['←', '↓', '→', '↑']  # Left, Down, Right, Up\n",
    "    \n",
    "    for i in range(rows):  # Iterate over rows first (Row-Major Order)\n",
    "        for j in range(cols):  # Then iterate over columns\n",
    "            state = i * cols + j  # Row-major order indexing\n",
    "            q_values = q_table[state]\n",
    "            best_action = np.argmax(q_values)  # Find the index of the best Q-value\n",
    "\n",
    "            # Draw grid cell\n",
    "            rect = patches.Rectangle((j, rows - i - 1), 1, 1, linewidth=1, edgecolor='black', facecolor='white')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Choose what to display in the center of the cell\n",
    "            if use_best_action:\n",
    "                center_text = action_labels[best_action]  # Show best action direction\n",
    "            else:\n",
    "                center_text = state_labels[state]  # Show environment label (F, H, G, etc.)\n",
    "\n",
    "            # Display the state label in the center of the cell\n",
    "            ax.text(j + 0.5, rows - i - 0.5, center_text, ha='center', va='center', \n",
    "                    fontsize=12, fontweight='bold', color='red' if center_text == 'H' else 'blue')\n",
    "\n",
    "            # Position Q-values within the cell (highlight best in bold)\n",
    "            for action_idx, (dx, dy) in enumerate([(0.2, -0.5), (0.5, -0.85), (0.75, -0.5), (0.5, -0.15)]):\n",
    "                value_text = f\"{action_labels[action_idx]} {q_values[action_idx]:.2f}\"\n",
    "                font_weight = 'bold' if action_idx == best_action else 'normal'  # Highlight best Q-value\n",
    "                ax.text(j + dx, rows - i + dy, value_text, ha='center', fontsize=10, fontweight=font_weight)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try it with our q-table\n",
    "Note that all the q-values are 0 at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our display function to visualize the Q-table\n",
    "visualize_q_table(Qtable_frozenlake, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the epsilon-greedy policy and the greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(env, state):\n",
    "    \"\"\"\n",
    "    Given an OpenAI Gymnasium environment and current state, \n",
    "    determines valid actions that can be taken from a given state in a grid-based environment.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The OpenAI Gymnasium environment.\n",
    "    - state: The current state index (integer).\n",
    "\n",
    "    Returns:\n",
    "    - A list of valid action indices (0: left, 1: down, 2: right, 3: up).\n",
    "    \"\"\"\n",
    "    rows, cols = env.unwrapped.desc.shape  # Get grid dimensions\n",
    "    row, col = divmod(state, cols)  # Convert state index to (row, col)\n",
    "\n",
    "    valid_actions = []\n",
    "\n",
    "    action_offsets = {\n",
    "        0: (0, -1),  # Left\n",
    "        1: (1, 0),   # Down\n",
    "        2: (0, 1),   # Right\n",
    "        3: (-1, 0)   # Up\n",
    "    }\n",
    "\n",
    "\n",
    "    for action, (dr, dc) in action_offsets.items():\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "\n",
    "        if 0 <= new_row < rows and 0 <= new_col < cols:  # Check if inside grid bounds\n",
    "            new_state = new_row * cols + new_col\n",
    "            if env.unwrapped.desc[new_row, new_col] != b'H':  # Prevent moving into holes\n",
    "                valid_actions.append(action)\n",
    "\n",
    "    return valid_actions\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon, env):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_int = random.uniform(0,1)\n",
    "  # if random_int > greater than epsilon --> exploitation\n",
    "  if random_int > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action = np.argmax(Qtable[state])\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    # Get the list of valid actions\n",
    "    valid_actions = get_valid_actions(env, state)\n",
    "    # Randomly select an action from the list of valid actions\n",
    "    action = random.choice(valid_actions)\n",
    "    # action = random.randint(0, env.action_space.n - 1)\n",
    "\n",
    "  return action\n",
    "\n",
    "\n",
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action = np.argmax(Qtable[state])\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to train the Q-Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1) (40%) Your Task\n",
    "Your task is, given the previous functions, write the training function which should:\n",
    "\n",
    "Go through a number of episodes. In each episode, \n",
    "<ul>\n",
    "    <li> reduce the epsilon appropriately because we want to lessen the exploration possibilities as we go on\n",
    "    <li> reset the environment and put us at the beginning. For the frozen lake, you would use <pre>state = env.reset()[0]</pre> because reset returns an array of things. We want the first element.\n",
    "    <li> Then, for this episode, go a number of steps to see if we find the goal state\n",
    "        <ul>\n",
    "        <li> Choose an action to take according to which policy you are using: greedy or epsilon_greedy\n",
    "        <li> Take that step in the environment using <pre>new_state, reward, done, truncated, info = env.step(action)</pre> which returns the new state and reward and whether or not you are done.\n",
    "        <li> Use the returned values to update your QTable using the Bellman Equation\n",
    "        <li> Update your state variable\n",
    "        </ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_update(Qtable, state, action, reward, next_state, gamma, learning_rate):\n",
    "  # You will need to implement the Bellman function here \n",
    "\n",
    "\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model - the end result will be our updated Q-Table\n",
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, learning_rate, gamma, env, max_steps, Qtable):\n",
    "\n",
    "    #state = env.reset()[0] # reset returns a tuple, but we only need the array\n",
    "\n",
    "\n",
    "    # Take action and observe Rt+1 and St+1\n",
    "    # Take the action (action) and observe the outcome state(new_state)) and reward (reward)\n",
    "    # new_state, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the hyperparameters we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 100    # Total training episodes\n",
    "learning_rate = 0.85         # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
    "max_steps = 100              # Max steps per episode\n",
    "gamma = 0.98                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability\n",
    "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, learning_rate, gamma, env, max_steps, Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the trained Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_q_table(Qtable_frozenlake, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the model perform on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent by running the environment for n_eval_episodes\n",
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param Q: The Q-table\n",
    "  :param seed: The evaluation seed array (for taxi-v3)\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in range(n_eval_episodes):\n",
    "    if seed:\n",
    "      state = env.reset(seed=seed[episode])[0] # reset returns a tuple, but we only need the array\n",
    "    else:\n",
    "      state = env.reset()[0] # reset returns a tuple, but we only need the array\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      # Take the action (index) that have the maximum expected future reward given that state\n",
    "      action = np.argmax(Q[state][:])\n",
    "      new_state, reward, done, truncated, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our Agent\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2) (10%) Training vs Reward\n",
    "Write a function to plot the mean_reward vs training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5%) Discussion\n",
    "How did the model do? What happens when you change the number of iterations it trains? How does the final QTable differ from the initial training of 100 iterations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Your Discussion goes here ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3) (40%) Now let's do it on a slippery lake\n",
    "The slippery lake makes it so sometimes you don't go the way you think you will. We will need to do it with a custom map. It won't do very well with the default map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and show the custom map\n",
    "custom_map=[\"SFFF\", \"FFHF\", \"FFFH\", \"HFFG\"]\n",
    "env = gym.make(\"FrozenLake-v1\", desc=custom_map, is_slippery=True, render_mode='rgb_array')\n",
    "env.reset()\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the parameters, train the model, and evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, play with the map and see what happens.\n",
    "You will need to define a new map and setup a new QTable and then train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5%) Discussion\n",
    "What did you learn? What happens when you played with the map on the slippery lake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Your discussion goes here ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
