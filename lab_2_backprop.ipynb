{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVL7_bgmIAPR"
   },
   "source": [
    "# Backpropagation Lab\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6ZbYjZZZ_yLV"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCcEPx5VIORj"
   },
   "source": [
    "## 1. (40%) Correctly implement and submit your own code for the backpropagation algorithm. \n",
    "\n",
    "## Code requirements \n",
    "- Ability to create a network structure with at least one hidden layer and an arbitrary number of nodes.\n",
    "- Random weight initialization with small random weights with mean of 0 and a varience of 1.\n",
    "- Use Stochastic/On-line training updates: Iterate and update weights after each training instance (i.e. do not attempt batch updates)\n",
    "- Implement a validation set based stopping criterion.\n",
    "- Shuffle training set at each epoch.\n",
    "- Option to include a momentum term\n",
    "\n",
    "You may use your own random train/test split or use the scikit-learn version if you want.\n",
    "\n",
    "Use your Backpropagation algorithm to solve the Debug data. We provide you with several parameters, and you should be able to replicate our results every time. When you are confident it is correct, run your script on the Evaluation data with the same parameters, and include your final weights in your report PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_a2KSZ_7AN0G"
   },
   "outputs": [],
   "source": [
    "class PerceptronClassifier(BaseEstimator,ClassifierMixin):\n",
    "\n",
    "    def __init__(self, lr=.1, shuffle=True):\n",
    "        \"\"\" \n",
    "            Initialize class with chosen hyperparameters.\n",
    "        Args:\n",
    "            lr (float): A learning rate / step size.\n",
    "            shuffle: Whether to shuffle the training data each epoch. DO NOT \n",
    "            SHUFFLE for evaluation / debug datasets.\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def fit(self, X, y, initial_weights=None):\n",
    "        \"\"\" \n",
    "            Fit the data; run the algorithm and adjust the weights to find a \n",
    "            good solution\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding\n",
    "            targets\n",
    "            y (array-like): A 2D numpy array with the training targets\n",
    "            initial_weights (array-like): allows the user to provide initial \n",
    "            weights\n",
    "        Returns:\n",
    "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
    "        \"\"\"\n",
    "        self.initial_weights = self.initialize_weights() if not initial_weights else initial_weights\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "            Predict all classes for a dataset X\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding \n",
    "            targets\n",
    "        Returns:\n",
    "            array, shape (n_samples,)\n",
    "                Predicted target values per element in X.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "\n",
    "        return [0]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" \n",
    "            Return accuracy of model on a given dataset. Must implement own \n",
    "            score function.\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with data, excluding targets\n",
    "            y (array-like): A 2D numpy array with targets\n",
    "        Returns:\n",
    "            score : float\n",
    "                Mean accuracy of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _shuffle_data(self, X, y):\n",
    "        \"\"\" \n",
    "            Shuffle the data! This _ prefix suggests that this method should \n",
    "            only be called internally.\n",
    "            It might be easier to concatenate X & y and shuffle a single 2D \n",
    "            array, rather than shuffling X and y exactly the same way, \n",
    "            independently.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    ### Not required by sk-learn but required by us for grading. Returns the weights.\n",
    "    def get_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KibCIXIThpbE"
   },
   "source": [
    "## 1.1 Debug \n",
    "\n",
    "Debug your model by running it on the [Debug Dataset](https://raw.githubusercontent.com/rmorain/CS472-1/master/datasets/perceptron/linsep2nonorigin.arff)\n",
    "\n",
    "Parameters:\n",
    "\n",
    "Learning Rate = 0.1\\\n",
    "Momentum = 0.5\\\n",
    "Deterministic = 10 [This means run it 10 epochs and should be the same everytime you run it]\\\n",
    "Shuffle = False\\\n",
    "Validation size = 0\n",
    "Initial Weights = All zeros\n",
    "\n",
    "---\n",
    "\n",
    "Expected Results: The weights do not need to be in this order or shape.\n",
    "\n",
    "Results if the # of outputs nodes = 1 AKA binary Node.\n",
    "\n",
    "debug_bp_0.csv\n",
    "\n",
    "Results if the # of outputs nodes = 2 Using a One Hot Encoding.\n",
    "\n",
    "debug_bp_2outs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgAyy82gixIF"
   },
   "outputs": [],
   "source": [
    "# Load debug data\n",
    "\n",
    "# Train on debug data\n",
    "\n",
    "# Check weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kY3VNB1ui03N"
   },
   "source": [
    "## 1.2 Evaluation\n",
    "\n",
    "We will evaluate your model based on it's performance on the [Evaluation Dataset](https://raw.githubusercontent.com/rmorain/CS472-1/master/datasets/perceptron/data_banknote_authentication.arff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2yAxA78QjDh2"
   },
   "outputs": [],
   "source": [
    "# Load evaluation data\n",
    "\n",
    "# Train on evaluation data\n",
    "\n",
    "# Print weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vWiTdlbR2Xh"
   },
   "source": [
    "## 2. (13%) Backpropagation on the Iris Classification problem.\n",
    "\n",
    "Load the Iris Dataset [Iris Dataset](https://raw.githubusercontent.com/rmorain/CS472-1/master/datasets/perceptron/iris.arff)\n",
    "\n",
    "Parameters:\n",
    "- One layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
    "- Use a 75/25 split of the data for the training/test set.\n",
    "- Use a learning rate of 0.1\n",
    "- Use a validation set taken from the training set for your your stopping criteria\n",
    "- Create one graph with the MSE (mean squared error) on the training set, the MSE on the VS, and the classification accuracy (% classified correctly) of the VS on the y-axis, and number of epochs on the x-axis. (Note there are two scales on the y-axis).\n",
    "\n",
    "The results for the different measurables should be shown with a different color, line type, etc. Typical backpropagation accuracies for the Iris data set are 85-95%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SSoasDQSKXb"
   },
   "outputs": [],
   "source": [
    "# Iris Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x"
   },
   "source": [
    "## 3. (4%) Working with the Vowel Dataset - Learning Rate\n",
    "\n",
    "Load the Vowel Dataset [Vowel Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff)\n",
    "\n",
    "- Use one layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
    "- Use random 75/25 splits of the data for the training/test set.\n",
    "- Try some different learning rates (LR). \n",
    "- For each LR find the best VS solution (in terms of VS MSE).\n",
    "- Create one graph with MSE for the training set, VS, and test set, at your chosen VS stopping epoch for each tested learning rate on the x-axis.  \n",
    "- Create another graph showing the number of epochs needed to get to the best VS solution on the y-axis for each tested learning rate on the x-axis. \n",
    "\n",
    "In general, whenever you are testing a parameter such as LR, # of hidden nodes, etc., test values until no more improvement is found. For example, if 20 hidden nodes did better than 10, you would not stop at 20, but would try 40, etc., until you no longer get improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KBGUn43ASiXW"
   },
   "outputs": [],
   "source": [
    "# Train on each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOteTlV6S0bq"
   },
   "source": [
    "## 3.1 (8%) Working with the Vowel Dataset - Intuition\n",
    "- Discuss the effect of varying learning rates. \n",
    "- Discuss why the vowel data set might be more difficult and report the baseline accuracy. \n",
    "- Consider which of the given input features you should actually use (Train/test, speaker, gender, ect) and discuss why you chose the ones you did.\n",
    "\n",
    "Typical backpropagation accuracies for the Vowel data set are above 75%.\n",
    "\n",
    "\n",
    "Note that each LR will probably require a different number of epochs to learn.\n",
    "\n",
    "\n",
    "Also note that the proper approach in this case would be to average the results of multiple random initial conditions (splits and initial weight settings) for each learning rate. To minimize work you may just do each learning rate once with the same initial conditions.\n",
    "\n",
    "\n",
    "If you would like you may average the results of multiple initial conditions (e.g. 3) per LR, and that obviously would give more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmq9GSbJS8k2"
   },
   "source": [
    "*Discuss intuition here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3F0Qp-BVi1R"
   },
   "source": [
    "*Explanation goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x"
   },
   "source": [
    "## 3.2 (10%) Working with the Vowel Dataset - Hidden Layer Nodes\n",
    "\n",
    "Using the best LR you discovered, experiment with different numbers of hidden nodes.\n",
    "\n",
    "- Start with 1 hidden node, then 2, and then double them for each test until you get no more improvement in accuracy. \n",
    "- For each number of hidden nodes find the best VS solution (in terms of VS MSE).  \n",
    "- Create one graph with MSE for the training set, VS, and test set, on the y-axis and # of hidden nodes on the x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmq9GSbJS8k2"
   },
   "source": [
    "*Discuss Hidden Layer Nodes here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3F0Qp-BVi1R"
   },
   "source": [
    "*Explanation goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x"
   },
   "source": [
    "## 3.3 (10%) Working with the Vowel Dataset - Momentum\n",
    "\n",
    "Try some different momentum terms in the learning equation using the best number of hidden nodes and LR from your earlier experiments.\n",
    "\n",
    "- Graph as in step 3.2 but with momentum on the x-axis and number of epochs until VS convergence on the y-axis.\n",
    "- You are trying to see how much momentum speeds up learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmq9GSbJS8k2"
   },
   "source": [
    "*Discuss momentum here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3F0Qp-BVi1R"
   },
   "source": [
    "*Explanation goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBmeNQ7jvcQ"
   },
   "source": [
    "## 4.1 (10%) Use the scikit-learn (SK) version of the MLP classifier on the iris and vowel data sets.  \n",
    "\n",
    "You do not need to go through all the steps above, nor graph results. Compare results (accuracy and learning speed) between your version and theirs for some selection of hyper-parameters. Try different hyper-parameters and comment on their effect.\n",
    "\n",
    "At a minimum, try\n",
    "\n",
    "- number of hidden nodes and layers\n",
    "- different activation functions\n",
    "- learning rate\n",
    "- regularization and parameters\n",
    "- momentum (and try nesterov)\n",
    "- early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OFQv70W2VyqJ"
   },
   "outputs": [],
   "source": [
    "# Load sklearn perceptron\n",
    "\n",
    "# Train on voting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqSFAXwlk3Ms"
   },
   "source": [
    "*Record impressions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBmeNQ7jvcQ"
   },
   "source": [
    "## 4.2 (5%) Pick a data set of your choice and learn it with the SK MLP version. \n",
    "- Use a grid and/or random search approach across a reasonable subset of hyper-parameters from the above \n",
    "- Report your best accuracy and hyper-parameters for your chosen data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OFQv70W2VyqJ"
   },
   "outputs": [],
   "source": [
    "# Load sklearn perceptron\n",
    "\n",
    "# Train on voting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTlK-kijk8Mg"
   },
   "source": [
    "## 5. (Optional 5% Extra credit) For the vowel data set use both the grid and random search approaches to find the hyper-parameters LR, # of hidden nodes, and momentum.  \n",
    "\n",
    "- Compare and discuss the values found with the ones you found in steps 3-5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional grid and random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqSFAXwlk3Ms"
   },
   "source": [
    "*Discuss findings here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab 1 - perceptron",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
