{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# Unsupervised Learning: Clustering Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import arff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCcEPx5VIORj"
      },
      "source": [
        "## 1. Initial practice with the K-Means and HAC algorithms\n",
        "Normalized inputs are in general important for clustering, but do not normalize for this task. For both algorithms:\n",
        "\n",
        "### 1.1 (10%) K-Means\n",
        "Run K-Means on this [Abalone Dataset.](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff)\n",
        "The dataset was modified to be smaller. The last datapoint should be on line 359 or the point 0.585,0.46,0.185,0.922,0.3635,0.213,0.285,10. The remaining points are commented out. Treat the output class (last column) as an additional input feature. Create your kmeans model with the paramaters KMeans(n_clusters=3, random_state=1, init='random', n_init=1) \n",
        "\n",
        "Output the follwing:\n",
        "- Class label for each point (labels_)\n",
        "- The k=3 cluster centers (cluster_centers_)\n",
        "- Number of iterations it took to converge (n_iter_)\n",
        "- Total sum squared error of each point from its cluster center (inertia_)\n",
        "- The total average silhouette score (see sklearn.metrics silhouette_score)\n",
        "\n",
        "Your results should be:\\\n",
        "[2 1 1 1 1 1 0 2 1 0 2 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 2 2 1 1 2 0 0 2 1 2\n",
        " 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 2 1 0 1\n",
        " 2 2 1 1 1 2 1 2 2 0 2 2 2 1 1 2 2 2 2 1 2 2 1 1 1 1 1 2 2 1 2 2 1 1 1 1 1\n",
        " 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1\n",
        " 1 1 2 1 1 1 2 1 2 2 2 2 2 2 2 0 2 2 0 0 2 2 2 1 1 1 1 1 1 1 1 1 2 0 1 1 2\n",
        " 2 2 1 1 1 2 2 2 1 1 2 1 2 2 1]\\\n",
        "[[ 0.64538462  0.51153846  0.17115385  1.52261538  0.557       0.29480769\n",
        "   0.55884615 19.23076923]\n",
        " [ 0.45437984  0.35383721  0.11709302  0.53466667  0.22172481  0.12295349\n",
        "   0.16694961  8.58914729]\n",
        " [ 0.59491379  0.47025862  0.16612069  1.14481897  0.44075     0.24400862\n",
        "   0.37537069 13.60344828]]\\\n",
        "3\\\n",
        "576.3523243807703\\\n",
        "0.5589106353312348"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kmeans with Abalone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Discussion*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KibCIXIThpbE"
      },
      "source": [
        "### 1.2 (10%) Hierarchical Agglomerative Clustering (HAC) \n",
        "\n",
        "Run HAC on the same [Abalone Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff) using complete linkage and k=3.\n",
        "\n",
        "Output the following:\n",
        "- Class label for each point (labels_)\n",
        "- The total average silhouette score\n",
        "\n",
        "Your results should be:\\\n",
        "[1 0 0 0 0 0 2 1 0 2 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 2 2 1 0 1\n",
        " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 2 1\n",
        " 1 1 0 0 1 1 0 1 1 2 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0\n",
        " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 2 2 2 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
        " 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 2 1 1 2 2 1 1 1 0 0 0 0 0 0 0 0 1 1 2 0 0 1\n",
        " 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0]\\\n",
        "0.5398112398376158"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HAC with Abalone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Discussion*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2. K-Means Clustering with the [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff)\n",
        "Don't include the output label as one of the input features\n",
        "\n",
        "### 2.1 (20%) K-Means Initial Centroids Experiments\n",
        "- Run K-Means 5 times with *k*=4, each time with different initial random centroids and with n_init=1.  Give inertia and silhouette scores for each run and discuss any variations in the results.\n",
        "- SKlearn has a parameter that does this automatically (n_init).  Try it out and discuss what you see.\n",
        "- Sklearn also has a parameter (init:'k-means++') which runs a simpler fast version of K-Means first on the data to come up with good initial centroids, and then runs the regular K-Means.  Try it out (with n_init = 1) and discuss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SSoasDQSKXb"
      },
      "outputs": [],
      "source": [
        "# K-Means initial centroid experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjkatnQY-Jep"
      },
      "source": [
        "Results and Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGfrL7p5-Jeq"
      },
      "source": [
        "### 2.2 (20%) Silhouette Graphs\n",
        "In this part you will show silhouette graphs for different *k* values.  Install the [Yellowbrick visualization package](https://www.scikit-yb.org/en/latest/quickstart.html) and import the [Silhouette Visualizer](https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html).  This library includes lots of visualization packages which you might find useful. (Note: The YellowBrick silhouette visualizer does not currently support HAC).\n",
        "- Show Silhouette graphs for clusterings with *k* = 2-6. Print the SSE (inertia) and total silhouette score for each.\n",
        "- Learn with the default n_init = 10 to help insure a decent clustering.\n",
        "- Using the silhouette graphs choose which *k* you think is best and discuss why. Think about and discuss more than just the total silhouette score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gigfanaW-Jeq"
      },
      "outputs": [],
      "source": [
        "# Iris Clustering with K-means and silhouette graphs\n",
        "from yellowbrick.cluster import SilhouetteVisualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IELq_zlu-Jeq"
      },
      "source": [
        "Discuss your results and justify which clustering is best based on the silhouette graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 (20%) Iris Clustering with HAC\n",
        "\n",
        "- Use the same dataset as above and learn with HAC clustering\n",
        "- Create one table with silhouette scores for k=2-6 for each of the linkage options single, average, complete, and ward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#HAC with Iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Discussion and linkage comparison*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmeNQ7jvcQ"
      },
      "source": [
        "## 4 (20%) Run both algorithms on a real world data\n",
        "- Choose any real world data set which you have not used previously\n",
        "- Use parameters of your choosing\n",
        "- Output one typical example of labels and silhouette scores for each algorithm\n",
        "- Show the silhouette graph for at least one reasonable *k* value for K-Means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFQv70W2VyqJ"
      },
      "outputs": [],
      "source": [
        "# Run both algoriths on a data set of your choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-oKHPPT-Jer"
      },
      "source": [
        "*Discussion and comparison*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extra Credit for Coding Your Own Clustering Algorithms\n",
        "### 5.1 (Optional 10% extra credit) Code up the K-Means clustering algorithm \n",
        "Below is a scaffold you could use if you want. As above, you only need to support numeric inputs, but think about how you would support nominal inputs and unknown values. Requirements for this task:\n",
        "- Your model should support the methods shown in the example scaffold below.\n",
        "- Ability to choose *k* and specify the *k* initial centroids.\n",
        "- Run and show the cluster label for each point with both the Iris data set and the data set of your choice above.\n",
        "\n",
        "### 5.2 (Optional 10% extra credit) Code up the HAC clustering algorithm \n",
        "\n",
        "- Your model should support the methods shown in the example scaffold below.\n",
        "- HAC should support both single link and complete link options.\n",
        "- HAC automatically generates all clusterings from *n* to 2.  You just need to output results for the curent chosen *k*.\n",
        "- Run and show the cluster label for each point with both the Iris data set and the data set of your choice above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion and comparision of each model implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin, ClusterMixin\n",
        "\n",
        "class KMEANSClustering(BaseEstimator,ClusterMixin):\n",
        "\n",
        "    def __init__(self,k=3,debug=False): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            k = how many final clusters to have\n",
        "            debug = if debug is true use the first k instances as the initial centroids otherwise choose random points as the initial centroids.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.debug = debug\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data\n",
        "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        return self\n",
        "    \n",
        "    def print_labels(self): # Print the cluster label for each data point\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HACClustering(BaseEstimator,ClusterMixin):\n",
        "\n",
        "    def __init__(self,k=3,link_type='single'): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            k = how many final clusters to have\n",
        "            link_type = single or complete. when combining two clusters use complete link or single link\n",
        "        \"\"\"\n",
        "        self.link_type = link_type\n",
        "        self.k = k\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data\n",
        "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        return self\n",
        "    \n",
        "    def print_labels(self): # Print the cluster label for each data point\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
