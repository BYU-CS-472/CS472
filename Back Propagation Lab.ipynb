{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVL7_bgmIAPR"
   },
   "source": [
    "# Backpropagation Lab\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6ZbYjZZZ_yLV"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCcEPx5VIORj"
   },
   "source": [
    "## 1. Correctly implement and submit your own code for the backpropagation algorithm. \n",
    "\n",
    "## Code requirements \n",
    "- Ability to create a network structure with at least one hidden layer and an arbitrary number of nodes.\n",
    "- Random weight initialization with small random weights with 0 mean. Remember that every hidden and output node should have its own bias weight.\n",
    "- Use stochastic training updates: update weights after each training instance (i.e. not batch)\n",
    "- Implement a validation set based stopping criterion. Keep your *BSSF* and once you go *w* epochs with no improvement (e.g. 10) then use the weights from the *BSSF*.\n",
    "- Option to include a momentum term\n",
    "\n",
    "Use your Backpropagation algorithm to solve the Debug data. We provide you with several parameters, and you should be able to replicate our results every time. When you are confident it is correct, run your script on the Evaluation data with the same parameters, and print your final weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_a2KSZ_7AN0G"
   },
   "outputs": [],
   "source": [
    "class MLP(BaseEstimator,ClassifierMixin):\n",
    "\n",
    "    def __init__(self,lr=.1, momentum=0, shuffle=True,hidden_layer_widths=None):\n",
    "        \"\"\" Initialize class with chosen hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            lr (float): A learning rate / step size.\n",
    "            shuffle(boolean): Whether to shuffle the training data each epoch. DO NOT SHUFFLE for evaluation / debug datasets.\n",
    "            momentum(float): The momentum coefficent \n",
    "        Optional Args (Args we think will make your life easier):\n",
    "            hidden_layer_widths (list(int)): A list of integers which defines the width of each hidden layer if hidden layer is none do twice as many hidden nodes as input nodes. (and then one more for the bias node)\n",
    "            For example: input width 1, then hidden layer will be 3 nodes\n",
    "        Example:\n",
    "            mlp = MLP(lr=.2,momentum=.5,shuffle=False,hidden_layer_widths = [3,3]),  <--- this will create a model with two hidden layers, both 3 nodes wide\n",
    "        \"\"\"\n",
    "        self.hidden_layer_widths\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "\n",
    "    def fit(self, X, y, initial_weights=None):\n",
    "        \"\"\" Fit the data; run the algorithm and adjust the weights to find a good solution\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "            y (array-like): A 2D numpy array with the training targets\n",
    "        Optional Args (Args we think will make your life easier):\n",
    "            initial_weights (array-like): allows the user to provide initial weights\n",
    "        Returns:\n",
    "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
    "\n",
    "        \"\"\"\n",
    "        self.weights = self.initialize_weights() if not initial_weights else initial_weights\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict all classes for a dataset X\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "        Returns:\n",
    "            array, shape (n_samples,)\n",
    "                Predicted target values per element in X.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return [0]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with data, excluding targets\n",
    "            y (array-like): A 2D numpy array with targets\n",
    "\n",
    "        Returns:\n",
    "            score : float\n",
    "                Mean accuracy of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _shuffle_data(self, X, y):\n",
    "        \"\"\" Shuffle the data! This _ prefix suggests that this method should only be called internally.\n",
    "            It might be easier to concatenate X & y and shuffle a single 2D array, rather than\n",
    "             shuffling X and y exactly the same way, independently.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    ### Not required by sk-learn but required by us for grading. Returns the weights.\n",
    "    def get_weights(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KibCIXIThpbE"
   },
   "source": [
    "## 1.1 Debug and Evaluation\n",
    "\n",
    "Debug your model using the following parameters:\n",
    "\n",
    "Learning Rate = 0.1\\\n",
    "Momentum = 0.5\\\n",
    "Deterministic = 10 [This means run it 10 epochs and should be the same everytime you run it]\\\n",
    "Shuffle = False\\\n",
    "Validation size = 0\\\n",
    "Initial Weights = All zeros\\\n",
    "Hidden Layer Widths = [4]\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1.1 (20%) Debug \n",
    "\n",
    "Debug your model by running it on the [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/linsep2nonorigin.arff)\n",
    "\n",
    "\n",
    "Expected Results for Binary Classification (i.e. 1 output node): [debug_bp_0.csv](https://github.com/cs472ta/CS472/blob/master/debug_solutions/debug_bp_0.csv) \n",
    "\n",
    "$$ \\text{Layer 1} = \\begin{bmatrix} -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 \\\\ 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 \\\\ -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 \\end{bmatrix}$$\n",
    "                                             \n",
    "$$ \\text{Layer 2} = \\begin{bmatrix} -0.01060888 \\\\ -0.01060888 \\\\ -0.01060888 \\\\ -0.01060888 \\\\ -0.02145495 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Expected Results for One Hot Vector Classification (i.e. 2 output nodes): [debug_bp_2outs.csv](https://github.com/cs472ta/CS472/blob/master/debug_solutions/debug_bp_2outs.csv) \n",
    "\n",
    "$$ \\text{Layer 1} = \\begin{bmatrix} -0.00018149 & -0.00018149 & -0.00018149 & -0.00018149 \\\\ 0.00157468 & 0.00157468 & 0.00157468 & 0.00157468 \\\\ -0.00788218 & -0.00788218 & -0.00788218 & -0.00788218 \\end{bmatrix}$$\n",
    "                          \n",
    "$$ \\text{Layer 2} = \\begin{bmatrix} 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.02148778 & -0.02148778 \\end{bmatrix}$$\n",
    "\n",
    "Based on how you index your weights, they may not be in the exact order or shape as our two examples above, but the weight values should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgAyy82gixIF"
   },
   "outputs": [],
   "source": [
    "# Load debug data\n",
    "\n",
    "# Train on debug data\n",
    "\n",
    "# Print weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kY3VNB1ui03N"
   },
   "source": [
    "### 1.1.2 (20%) Evaluation\n",
    "\n",
    "Evaluate your model using the following parameters:\n",
    "\n",
    "Learning Rate = 0.1\\\n",
    "Momentum = 0.5\\\n",
    "Deterministic = 10 [This means run it 10 epochs and should be the same everytime you run it]\\\n",
    "Shuffle = False\\\n",
    "Validation size = 0\\\n",
    "Initial Weights = All zeros\\\n",
    "Hidden Layer Widths = [4]\n",
    "\n",
    "We will evaluate your model based on the weights your code prints after training on the [Evaluation Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/data_banknote_authentication.arff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2yAxA78QjDh2"
   },
   "outputs": [],
   "source": [
    "# Load evaluation data\n",
    "\n",
    "# Train on evaluation data\n",
    "\n",
    "# Print weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vWiTdlbR2Xh"
   },
   "source": [
    "## 2. (10%) Backpropagation on the Iris Classification problem.\n",
    "\n",
    "Train on the Iris Dataset [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff). For this and all following experiments, always start with random weights and shuffle the training set before each epoch. You may use your own data shuffling code and random train/test split code or use scikit-learn versions if you prefer.\n",
    "\n",
    "Parameters:\n",
    "- One layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
    "- Use a random 80/20 split of the data for the training/test set.\n",
    "- Use a learning rate of 0.1\n",
    "- Use a validation set (10-15% of the training set is common) taken from the training set for your stopping criteria. \n",
    "- Create one graph with MSE (mean squared error, *y*-axis) vs epochs (*x*-axis) for the MSEs on the training set, validation set, and test set. Always include the values at epoch 0 (your initial weights before training) - A graph example is coming asap\n",
    "- Create one graph with classification accuracy (% classified correctly) vs epochs from the training set, validation set, and test set\n",
    "\n",
    "The results for the different measurables should be shown with a different color, line type, etc. Typical backpropagation accuracies for the Iris data set are 85-95%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SSoasDQSKXb"
   },
   "outputs": [],
   "source": [
    "# Iris Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOteTlV6S0bq"
   },
   "source": [
    "## 3. Working with Hyperparameters \n",
    "### 3.1 (5%) Vowel Dataset - Intuition\n",
    "- In this section we use the vowel dataset to consider the hyperparameters of learning rate, number of hidden ndoes, and momentum.\n",
    "- Discuss why the vowel data set has lower accuracy than Iris.\n",
    "- Report both datasets' baseline accuracies. Baseline accuracy is what you would get if the model just outputs the majority class of the data set.\n",
    "- Consider which of the vowel dataset's given input features you should actually use (Train/test, speaker, gender, etc.) and discuss why you chose the ones you did.\n",
    "\n",
    "Typical backpropagation accuracies for the Vowel data set are above 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmq9GSbJS8k2"
   },
   "source": [
    "*Discuss these items here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x"
   },
   "source": [
    "### 3.2 (10%) Learning Rate\n",
    "Load the Vowel Dataset [Vowel Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff)\n",
    "\n",
    "- Use one layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
    "- Use random 75/25 splits of the data for the training/test set.\n",
    "- Use a validation set taken from the training set for your stopping criteria.\n",
    "- Try some different learning rates (LR). Note that each LR will require a different number of epochs to learn. \n",
    "- These hyperparamaters can effect both accuracy and time required for learning. For each LR you test, plot their validations set's MSE (*y*-axis) vs epochs (*x*-axis) on the same graph. Graph 4-5 different LRs and make them different enough to see a difference between them. For stopping use a window of 5 epochs since no change from *BSSF*. 5 is smaller than typical but fine for demonstrating and viewing the concept here. Show the values for the 5 epochs past *BSSF* but put a red point at the *BSSF* epoch to make it clear which model you actually choose for each learning rate. Below is an example graph with made up data. We include the Matplotlib code which you may use as a template, but you may use any graphing tools you want. An improved graph example is coming asap.\n",
    "- Create a table which includes a row for the final chosen model for each LR, showing the LR, # epochs to learn the model, and the final MSE for the training set, validation set, and test set.\n",
    "\n",
    "In general, whenever you are testing a parameter such as LR, # of hidden nodes, etc., test values until no more improvement is found. For example, if 20 hidden nodes did better than 10, you would not stop at 20, but would try 40, etc., until you no longer get improvement.\n",
    "\n",
    "In real testing one averages the results of multiple trials per LR with different intitial conditions. That gives more accurate results but is not required though you are welcome to do so.\n",
    "\n",
    "<img src=https://raw.githubusercontent.com/cs472ta/CS472/master/images/backpropagation/backprop_val_set_MSE_vs_epochs.png width=500 height=500  align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KBGUn43ASiXW"
   },
   "outputs": [],
   "source": [
    "# Train with different learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss the effect of different learning rates from your graph and table*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x"
   },
   "source": [
    "### 3.3 (10%) Number of Hidden Nodes\n",
    "\n",
    "Using the best LR you discovered, experiment with different numbers of hidden nodes.\n",
    "\n",
    "- Start with 1 hidden node, then 2, and then double them for each test until you get no more improvement in accuracy. \n",
    "- For each number of hidden nodes find the best validation set solution (in terms of validation set MSE).  \n",
    "- Create both a graph and table just like above, except with # of hidden nodes rather than LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different numbers of hidden nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmq9GSbJS8k2"
   },
   "source": [
    "*Discuss the effect of different numbers of hidden nodes from your graph and table* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x"
   },
   "source": [
    "### 3.4 (10%) Momentum\n",
    "\n",
    "Try some different momentum terms between 0 and 1 using the best number of hidden nodes and LR from your earlier experiments.\n",
    "\n",
    "- Create both a graph and table just like above, except with different momentum values rather than LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different momentum values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmq9GSbJS8k2"
   },
   "source": [
    "*Discuss the effect of different momentum values from your graph and table*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scikit-learn Classifier and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBmeNQ7jvcQ"
   },
   "source": [
    "### 4.1 (10%) Use the scikit-learn (SK) version of the MLP classifier on the Iris data set and one data set of your choice.  \n",
    "\n",
    "You do not need to go through all the steps above, nor graph results. Compare results (accuracy and learning speed) between your version and theirs for some selection of hyper-parameters. Try some different hyper-parameters and comment on their effect.\n",
    "\n",
    "At a minimum, try\n",
    "\n",
    "- number of hidden nodes and layers\n",
    "- different activation functions\n",
    "- learning rate\n",
    "- regularization and parameters\n",
    "- momentum (and try nesterov)\n",
    "- early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OFQv70W2VyqJ"
   },
   "outputs": [],
   "source": [
    "# Load sklearn MLP\n",
    "\n",
    "# Train on Iris and one other data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqSFAXwlk3Ms"
   },
   "source": [
    "*Discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBmeNQ7jvcQ"
   },
   "source": [
    "### 4.2 (5%) Using the Iris Dataset automatically adjust hyper-parameters using your choice of grid or random search\n",
    "- Use a grid or random search approach across a reasonable subset of hyper-parameters from the above \n",
    "- Report your best accuracy and hyper-parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OFQv70W2VyqJ"
   },
   "outputs": [],
   "source": [
    "# Load sklearn MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTlK-kijk8Mg"
   },
   "source": [
    "### 5. (Optional 5% Extra credit) For the Iris data set, use the other hyper-parameter approach that you did not use in part 4.2 to find LR, # of hidden nodes, momentum, and any other hyperparamters you want to consider.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqSFAXwlk3Ms"
   },
   "source": [
    "*Discuss your results and any differences*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab 1 - perceptron",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
